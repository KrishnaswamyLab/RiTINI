{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "> data utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, numpy as np, pandas as pd, torch, itertools\n",
    "from typing import Callable, Union\n",
    "from gode.utils import get_device, torch_t\n",
    "\n",
    "def augment_with_time(\n",
    "    x:torch.Tensor, \n",
    "    t:int, size:int=1, \n",
    "    augment:bool=True\n",
    ") -> torch.Tensor:  \n",
    "    '''\n",
    "    Augment feature matrix x with zeros and time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x\n",
    "        The input features to augment.\n",
    "    \n",
    "    t\n",
    "        Time to append to x.\n",
    "\n",
    "    size\n",
    "        Number of columns of zeros to add to x.\n",
    "\n",
    "    augment\n",
    "        Whether or not to augment x with zeroes and time. If `False` returns x unchanged\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    augmented\n",
    "        The augmented tensor (x, t, zeros...).\n",
    "    '''\n",
    "    # Internally handle if / else statement\n",
    "    if not augment:\n",
    "        return x\n",
    "    \n",
    "    # Ensure t is wrapped as torch Tensor\n",
    "    t = torch_t(t, device=x.device)\n",
    "    \n",
    "    # Augment with size number of 0s\n",
    "    zeros = torch.zeros(x.size(dim=0), size).to(x.device)\n",
    "    \n",
    "    # Time is only concatenated once\n",
    "    times = t.repeat(x.size(dim=0), 1)\n",
    "        \n",
    "    augmented = torch.cat((x, times, zeros), dim=1)\n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def group_extract(\n",
    "    df:pd.DataFrame,\n",
    "    group:str, groupby:str='binned_sim_time', \n",
    "    index:str=None, set_index:bool=True, \n",
    "    as_df:bool=False\n",
    "):\n",
    "    '''\n",
    "    Gets group from a DataFrame\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Pandas DataFrame\n",
    "\n",
    "    group\n",
    "        group to extract\n",
    "    \n",
    "    groupby\n",
    "        key in df to group by\n",
    "\n",
    "    index\n",
    "        key in df to set to index\n",
    "\n",
    "    set_index\n",
    "        whether or not to set index\n",
    "\n",
    "    as_df\n",
    "        whether or not to return results in a DataFrame or NumPy array\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    group\n",
    "        records of df that have df.groupby == group \n",
    "    '''\n",
    "    # Get just the records in the corresponding group\n",
    "    df_g = df.groupby(groupby).get_group(group)\n",
    "    \n",
    "    # Set the requested index column\n",
    "    if index is not None and set_index:\n",
    "        df_g = df_g.set_index(index)\n",
    "    \n",
    "    # Just get the numpy array\n",
    "    if not as_df:\n",
    "        df_g = df_g.values\n",
    "    \n",
    "    return df_g\n",
    "\n",
    "\n",
    "def sample_group(\n",
    "    df:pd.DataFrame,\n",
    "    group:str, groupby:str='binned_sim_time', \n",
    "    index:str=None, set_index:bool=True, \n",
    "    as_df:bool=False,\n",
    "    size:int=100, replace:bool=False,\n",
    "    to_torch:bool=False, device=None\n",
    "):\n",
    "    '''\n",
    "    Samples a group from a DataFrame\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Pandas DataFrame\n",
    "\n",
    "    group\n",
    "        group to extract\n",
    "    \n",
    "    groupby\n",
    "        key in df to group by\n",
    "\n",
    "    index\n",
    "        key in df to set to index\n",
    "\n",
    "    set_index\n",
    "        whether or not to set index\n",
    "\n",
    "    as_df\n",
    "        whether or not to return results in a DataFrame or NumPy array\n",
    "\n",
    "    size\n",
    "        number of records to retrieve\n",
    "\n",
    "    replace\n",
    "        whether or not to sample with replacement\n",
    "\n",
    "    to_torch\n",
    "        whether or not to cast to `torch.Tensor`\n",
    "\n",
    "    device\n",
    "        which device to put results on if `to_tensor=True`\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sampled\n",
    "        `size` records of df that have df.groupby == group \n",
    "    '''\n",
    "    sub = group_extract(df, group, groupby, index, set_index, as_df)\n",
    "    sampled = sub.sample(size, replace=replace)\n",
    "    if to_torch:\n",
    "        sampled = torch.tensor(sub)\n",
    "        if device is not None:\n",
    "            sampled = sub.to(device)\n",
    "    return sampled\n",
    "\n",
    "\n",
    "def sample_group_index(\n",
    "    df, group, groupby:str='binned_sim_time', size:int=100, replace:bool=False\n",
    ") -> np.array:\n",
    "    '''\n",
    "    Samples a group from a DataFrame\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Pandas DataFrame\n",
    "\n",
    "    group\n",
    "        group to extract\n",
    "    \n",
    "    groupby\n",
    "        key in df to group by\n",
    "\n",
    "    size\n",
    "        number of records to retrieve\n",
    "\n",
    "    replace\n",
    "        whether or not to sample with replacement\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    indicies\n",
    "        indicies of the sampled DataFrame\n",
    "    '''\n",
    "    df_sampled = sample_group(df, group, groupby, size=size, as_df=True, replace=replace)\n",
    "    return df_sampled.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def clean_features_and_drop_columns(df, features=None, drop_columns=[]):\n",
    "    '''\n",
    "    Given a DataFrame returns all columns in features not in drop_columns\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Pandas DataFrame\n",
    "\n",
    "    features\n",
    "        columns explicitly to keep. If None, then uses all of them.\n",
    "    \n",
    "    drop_columns\n",
    "       columns explicitly to drop. If [], then drops none of them.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    features\n",
    "        columns from features that are also in df\n",
    "\n",
    "    drop_columns\n",
    "        columns from drop_columns that are also in df\n",
    "    '''\n",
    "    # Get all known features in DataFrame\n",
    "    if features is None:\n",
    "        features = df.columns\n",
    "        \n",
    "    # Can only drop columns if they are in the DataFrame to begin with\n",
    "    drop_columns = [col for col in drop_columns if col in df.columns]\n",
    "    \n",
    "    # Can subset DataFrame columns by keeping columns we want, or dropping unwanted ones\n",
    "    # here we use a combined approach\n",
    "    features = [feature for feature in features if feature not in drop_columns]\n",
    "    return features, drop_columns\n",
    "\n",
    "\n",
    "def aggregate_df_group(\n",
    "    df_grouped, group, aggregation='mean', missing_value=0,\n",
    "    features=None, drop_columns=[]\n",
    "):\n",
    "    '''\n",
    "    Aggregates rows for a given group of a DataFrame\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_grouped\n",
    "        Pandas DataFrame grouped \n",
    "\n",
    "    group\n",
    "        group to extract\n",
    "    \n",
    "    aggregation\n",
    "        aggregation function to use e.g. mean, sum, etc\n",
    "\n",
    "    missing_value\n",
    "        What to fill in for each column if group does not exist.\n",
    "\n",
    "    features\n",
    "        columns of df explicitly to keep. If None, then uses all of them.\n",
    "    \n",
    "    drop_columns\n",
    "       columns of df explicitly to drop. If [], then drops none of them.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    group_agg\n",
    "        values of the aggregated group from the df\n",
    "    '''\n",
    "    if features is None:\n",
    "        features, drop_columns = clean_features_and_drop_columns(df_group, features, drop_columns)\n",
    "\n",
    "    # TODO: maybe fix sampling?\n",
    "    # Poor sample, missing a cluster group so just setting its value to all zeros\n",
    "    if group not in df_grouped.groups:\n",
    "        return [missing_value for feature in features]\n",
    "\n",
    "    # Ids of rows in current group\n",
    "    df_group = df_grouped.get_group(group)\n",
    "    group_idx = df_group.index\n",
    "\n",
    "    \n",
    "    # Filter df\n",
    "    # NOTE: .drop(columns=drop_columns) would be redundant, we already filtered\n",
    "    # those out for features\n",
    "    df_group = df_group.loc[group_idx, features]\n",
    "\n",
    "    # Aggregate across groups e.g. this is akin to df.mean()\n",
    "    group_agg = getattr(df_group, aggregation)()\n",
    "    return group_agg.values\n",
    "\n",
    "\n",
    "def sample_aggregate_group_at_t(\n",
    "    df, t, time_key:str='binned_sim_time', \n",
    "    size=100, replace:bool=False,\n",
    "    groupby:str='cell_type',  groups = None,\n",
    "    aggregation='mean', missing_value = 0, \n",
    "    features = None, drop_columns=[]\n",
    "):\n",
    "    '''\n",
    "    Aggregates rows for a given group of a DataFrame\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Pandas DataFrame \n",
    "\n",
    "    t\n",
    "        current time to extract\n",
    "    \n",
    "    time_key\n",
    "        column name of df corresponding to time\n",
    "\n",
    "    size\n",
    "        number of samples to extract\n",
    "\n",
    "    replace\n",
    "        whether or not to sample with replacement\n",
    "\n",
    "    groupby\n",
    "        key of df to group by.\n",
    "\n",
    "    groups\n",
    "        list of all known groups in groupby. If None will calculate\n",
    "            via df[groupby].unique()\n",
    "\n",
    "    aggregation\n",
    "        aggregation function to use e.g. mean, sum, etc\n",
    "\n",
    "    missing_value\n",
    "        What to fill in for each column if group does not exist.\n",
    "\n",
    "    features\n",
    "        columns of df explicitly to keep. If None, then uses all of them.\n",
    "    \n",
    "    drop_columns\n",
    "       columns of df explicitly to drop. If [], then drops none of them.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_groups_t\n",
    "        values of the aggregated groups of the df at t\n",
    "    '''\n",
    "    # Get all known clusters regardless of time\n",
    "    if groups is None:\n",
    "        groups = sorted(df[groupby].unique())\n",
    "   \n",
    "    features, drop_columns = clean_features_and_drop_columns(df, features, drop_columns)\n",
    "\n",
    "    # Get ids of the recprds in our sample\n",
    "    idx = sample_group_index(df, t, time_key, size, replace)\n",
    "    \n",
    "    # Group records from our sample to their cluster\n",
    "    df_grouped = df.loc[idx].groupby(groupby)\n",
    "    \n",
    "    # NOTE: could do this in list comprehension, but this is easier to read\n",
    "    # Bookkeeping variable\n",
    "    results = []\n",
    "    for group in groups:\n",
    "        group_agg = aggregate_df_group(\n",
    "            df_grouped, group, aggregation, missing_value,\n",
    "            features, drop_columns\n",
    "        )        \n",
    "        results.append(group_agg)\n",
    "    df_groups_t = pd.DataFrame(results, index=groups, columns=features)\n",
    "    return df_groups_t\n",
    "\n",
    "def representative_cell_types_at_t(\n",
    "    df_cells, t, time_key:str='binned_sim_time', \n",
    "    size=100, replace:bool=False,\n",
    "    groupby:str='cell_type',  groups = None,\n",
    "    aggregation='mean', missing_value = 0, \n",
    "    features = None, drop_columns=[]\n",
    "):\n",
    "    '''\n",
    "    Wrapper for sample_aggregate_group_at_t\n",
    "    '''\n",
    "    return sample_aggregate_group_at_t(\n",
    "        df_cells, t, time_key, size, replace, groupby, groups, \n",
    "        aggregation, missing_value, features, drop_columns\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def make_train_test_dataframe(df:pd.DataFrame, fraction:float=85/100):\n",
    "    df_train = df.sample(frac=85/100)\n",
    "    df_test = df.loc[~df.index.isin(df_train.index)]\n",
    "    return df_train, df_test\n",
    "\n",
    "def get_mean_features_per_group_and_time(\n",
    "    df:pd.DataFrame, \n",
    "    features:Union[list, np.ndarray],\n",
    "    time_key:str='binned_sim_time',\n",
    "    groupby:str='cell_type',\n",
    "):\n",
    "    df_mu = df.groupby([time_key, groupby])\\\n",
    "    .mean().filter(features)\\\n",
    "    .reset_index()\n",
    "    return df_mu\n",
    "\n",
    "def make_mean_data_ti(\n",
    "    df:pd.DataFrame, \n",
    "    features:Union[list, np.ndarray, pd.Series],\n",
    "    time_key:str='binned_sim_time',\n",
    "    groupby:str='cell_type',\n",
    "    device:torch.device=None,\n",
    "    time_bins:Union[list, np.ndarray, pd.Series]=None\n",
    "):\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "        \n",
    "    if time_bins is None:\n",
    "        time_bins = np.sort(df[time_key].unique())\n",
    "\n",
    "    groups = np.sort(df[groupby].unique())\n",
    "\n",
    "    # NOTE: sometimes throws ValueError: setting an array element with a sequence.\n",
    "    # for no reason\n",
    "    # df_mu = get_mean_features_per_group_and_time(df, features, time_key, groupby)\n",
    "    # data_ti = np.array([\n",
    "    #     df_mu[df_mu[time_key] == t].filter(features).values.astype(float)\n",
    "    #     for t in time_bins\n",
    "    # ]).astype(float)\n",
    "    # data_ti = torch.transpose(torch.Tensor(data_ti), 1, 2).to(device)\n",
    "\n",
    "    # NOTE: Replacement option 1. for loops\n",
    "    # res = []\n",
    "    # for t in time_bins:\n",
    "    #     df_t = df_mu[df_mu[time_key] == t]\n",
    "\n",
    "    #     for group in groups:\n",
    "    #         df_tc = df_t[df_t[groupby] == group]\n",
    "    #         df_tc = df_tc.filter(features)\n",
    "    #         if df_tc.empty:\n",
    "    #             values = [0 for feature in features]\n",
    "    #         else:\n",
    "    #             values = df_tc.values.flatten().tolist()\n",
    "    #             values\n",
    "    #         res.append(values)\n",
    "    # data_ti = np.array(res).astype(float)\n",
    "\n",
    "    # NOTE: Replacement option 2. itertools\n",
    "    df_g = df.groupby([time_key, groupby])\n",
    "    keys = itertools.product(time_bins, groups)    \n",
    "    res = np.empty(0)\n",
    "    for (t, group) in keys:\n",
    "        try:\n",
    "            values = df_g.get_group((t, group)).filter(features).mean().values\n",
    "        except KeyError:\n",
    "            values = np.array([0 for feature in features])\n",
    "        res = np.vstack((res, values)) if res.size else values\n",
    "    data_ti = np.array(res).astype(float).reshape(len(time_bins), len(groups), -1)\n",
    "            \n",
    "    data_ti = torch.transpose(torch.Tensor(data_ti), 1, 2).to(device)\n",
    "    return data_ti\n",
    "\n",
    "def get_data_ti(\n",
    "    df:pd.DataFrame, \n",
    "    t, \n",
    "    size:int,\n",
    "    features:Union[list, np.ndarray, pd.Series],\n",
    "    replace:bool=False,\n",
    "    time_key:str='binned_sim_time',\n",
    "    groupby:str='cell_type',\n",
    "    device:torch.device=None\n",
    "):\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "        \n",
    "    return torch.Tensor(\n",
    "        sample_aggregate_group_at_t(\n",
    "            df, t, time_key=time_key, \n",
    "            size=size, replace=replace,\n",
    "            groupby=groupby, features=features\n",
    "        ).values\n",
    "    ).to(device).T\n",
    "\n",
    "\n",
    "def stack_timepoints_into_dataframe(\n",
    "    data_ti:Union[list, np.ndarray, torch.Tensor],\n",
    "    genes:Union[list, np.ndarray, torch.Tensor],\n",
    "    cell_types:Union[list, np.ndarray, torch.Tensor],\n",
    "    transcription_factors:Union[list, np.ndarray, torch.Tensor],\n",
    "):\n",
    "    inner_cols = np.concatenate((genes, ['time']))\n",
    "    outer_cols = np.concatenate((['cell_type', 'time'], transcription_factors))\n",
    "    df = pd.DataFrame(\n",
    "            np.array([\n",
    "                pd.DataFrame(\n",
    "                    np.hstack((\n",
    "                        dt.T.detach().cpu().numpy(), \n",
    "                        np.repeat(i, len(cell_types)).reshape(-1, 1)\n",
    "                    )), columns=inner_cols, index=cell_types\n",
    "                )\n",
    "                    .reset_index()\n",
    "                    .rename(columns={'index':'cell_type'})\n",
    "                    .loc[:, outer_cols]\n",
    "                    .values\n",
    "                for i, dt in enumerate(data_ti)\n",
    "            ], dtype=object).reshape(-1, 2 + len(transcription_factors)),\n",
    "            columns=outer_cols\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def make_results_dataframe(\n",
    "    data_ti:Union[list, np.ndarray, torch.Tensor],\n",
    "    data_pi:Union[list, np.ndarray, torch.Tensor],\n",
    "    genes:Union[list, pd.Series, np.ndarray],\n",
    "    cell_types:Union[list, np.ndarray],\n",
    "    transcription_factors:Union[list, np.ndarray],\n",
    "):\n",
    "    df_true = stack_timepoints_into_dataframe(data_ti, genes, cell_types, transcription_factors)\n",
    "    df_pred = stack_timepoints_into_dataframe(data_pi, genes, cell_types, transcription_factors)\n",
    "    res = []\n",
    "    for kind, df_cur in zip(('ground_truth', 'prediction'), (df_true, df_pred)):\n",
    "        for row, record in df_cur.iterrows():\n",
    "            for tf in transcription_factors:\n",
    "                res.append({\n",
    "                    'cell_type': record.cell_type,\n",
    "                    'time': record.time,\n",
    "                    'tf': tf,\n",
    "                    'expression': record[tf],\n",
    "                    'type': kind\n",
    "                })\n",
    "    df_res = pd.DataFrame(res)\n",
    "    return df_res\n",
    "\n",
    " \n",
    "\n",
    "def make_results_dataframe(\n",
    "    data_ti:Union[list, np.ndarray, torch.Tensor],\n",
    "    data_pi:Union[list, np.ndarray, torch.Tensor],\n",
    "    genes:Union[list, pd.Series, np.ndarray],\n",
    "    cell_types:Union[list, np.ndarray],\n",
    "    transcription_factors:Union[list, np.ndarray],\n",
    "):\n",
    "    df_true = stack_timepoints_into_dataframe(data_ti, genes, cell_types, transcription_factors)\n",
    "    df_pred = stack_timepoints_into_dataframe(data_pi, genes, cell_types, transcription_factors)\n",
    "    df_pred.loc[:, 'time'] += 1\n",
    "    res = []\n",
    "    for kind, df_cur in zip(('ground_truth', 'prediction'), (df_true, df_pred)):\n",
    "        for row, record in df_cur.iterrows():\n",
    "            for tf in transcription_factors:\n",
    "                res.append({\n",
    "                    'cell_type': record.cell_type,\n",
    "                    'time': record.time,\n",
    "                    'tf': tf,\n",
    "                    'expression': record[tf],\n",
    "                    'type': kind\n",
    "                })\n",
    "    df_res = pd.DataFrame(res)\n",
    "    return df_res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from gode.utils import dearray\n",
    "def get_spearmanr_at_t_k(data_ti, data_tp, t=0, k=0, drop_one=True):\n",
    "    # NOTE: t = gene (TF factor), k = kind, \n",
    "    \n",
    "    # NOTE: start at one, because data_tp is for t=1-->t=N\n",
    "    #       whereas data_ti starts at t=0\n",
    "    n = 1 if drop_one else 0\n",
    "    arr_x = data_ti[n:, t, k]    \n",
    "    arr_y = data_tp[:, t, k]\n",
    "    s_cor = spearmanr(arr_x, arr_y)\n",
    "    return s_cor\n",
    "\n",
    "def get_spearmanr(\n",
    "    data_ti, data_tp, drop_one=True,\n",
    "    columns=None, index=None\n",
    "):\n",
    "    # shape for dyngen was like (50, 116, 2) i.e. (t, genes, cell types)\n",
    "    x = np.array(dearray(data_ti))\n",
    "    y = np.array(dearray(data_tp))\n",
    "    corrs = np.array([ \n",
    "        [\n",
    "           get_spearmanr_at_t_k(x, y, t, k, drop_one).correlation\n",
    "            for k in range(data_ti.shape[-1])    \n",
    "        ]\n",
    "        for t in range(data_ti.shape[1])\n",
    "    ])\n",
    "    return pd.DataFrame(corrs, columns=columns, index=index).replace(np.nan, 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
