{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dgl\n",
    "\n",
    "> Dgl functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from ast import Call\n",
    "import itertools, math, numpy as np, pandas as pd\n",
    "import dgl, dgl.function as fn\n",
    "\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torchdiffeq\n",
    "\n",
    "from typing import Callable, Union, Literal\n",
    "\n",
    "# NOTE: could try to make dynamic decorator that wraps over the function\n",
    "# class DGLLayer:\n",
    "#     def __init__(self, layer:Union[\n",
    "#         dgl.nn.pytorch.conv.GraphConv,\n",
    "#         dgl.nn.pytorch.conv.GATConv,\n",
    "#         dgl.nn.pytorch.conv.GATv2Conv,\n",
    "#         dgl.nn.pytorch.conv.TAGConv,\n",
    "#         dgl.nn.pytorch.conv.SAGEConv,\n",
    "#         dgl.nn.pytorch.conv.DOTGatConv,\n",
    "#         dgl.nn.pytorch.conv.PNAConv,\n",
    "#     ]):\n",
    "#         self.graph = graph\n",
    "#         self.layer = layer\n",
    "\n",
    "#     def __call__(self, graph:dgl.DGLGraph, *args, **kwargs):\n",
    "#         self.layer(*args, **kwargs)\n",
    "\n",
    "# class DGLGATConv(dgl.nn.pytorch.conv.GATConv):\n",
    "#     def __init__(self, graph:dgl.DGLGraph, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.g = graph\n",
    "        \n",
    "#     def forward(self, feat, get_attention=False):\n",
    "#         return super().forward(self.g, feat, get_attention)\n",
    "#dgl.nn.pytorch.conv.GraphConv,  in_feats, out_feats, norm='both', weight=True, bias=True, activation=None, allow_zero_in_degree=False\n",
    "#dgl.nn.pytorch.conv.TAGConv,  in_feats, out_feats, k=2, bias=True, activation=None\n",
    "#dgl.nn.pytorch.conv.GATConv,  (in_feats, out_feats, num_heads, feat_drop=0.0, attn_drop=0.0, negative_slope=0.2, residual=False, activation=None, allow_zero_in_degree=False, bias=True\n",
    "#dgl.nn.pytorch.conv.GATv2Conv,  (in_feats, out_feats, num_heads, feat_drop=0.0, attn_drop=0.0, negative_slope=0.2, residual=False, activation=None, allow_zero_in_degree=False, bias=True, share_weights=False)\n",
    "#dgl.nn.pytorch.conv.SAGEConv,  in_feats, out_feats, aggregator_type, feat_drop=0.0, bias=True, norm=None, activation=None\n",
    "#dgl.nn.pytorch.conv.PNAConv,  in_size, out_size, aggregators, scalers, delta, dropout=0.0, num_towers=1, edge_feat_size=0, residual=Tru\n",
    "#dgl.nn.pytorch.conv.DotGatConv,  (in_feats, out_feats, num_heads, allow_zero_in_degree=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/home/solstice/projects/gode/.conda/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /home/solstice/projects/gode/.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import dgl.nn.pytorch.conv as conv\n",
    "\n",
    "class MeanAttentionLayer(nn.Module):\n",
    "    def __init__(self, axis=1):\n",
    "        super().__init__()\n",
    "        self.axis = axis\n",
    "    def forward(self, x):\n",
    "        return torch.mean(x, axis=self.axis)\n",
    "\n",
    "class SumAttentionLayer(nn.Module):\n",
    "    def __init__(self, axis=1):\n",
    "        super().__init__()\n",
    "        self.axis = axis\n",
    "    def forward(self, x):\n",
    "        return torch.sum(x, axis=self.axis)\n",
    "\n",
    "class DGLSequential(nn.Module):\n",
    "    def __init__(self, graph, *args):\n",
    "        super(DGLSequential, self).__init__()\n",
    "        self.graph = graph\n",
    "        for idx, module in enumerate(args):\n",
    "            self.add_module(str(idx), module)\n",
    "\n",
    "    def forward(self, graph, input, **kwargs):\n",
    "        for module in self:\n",
    "            input = module(graph, input)\n",
    "        return input\n",
    "\n",
    "\n",
    "class DGLDotGatConv(conv.DotGatConv):\n",
    "    def __init__(self, graph, in_feats, out_feats, num_heads, allow_zero_in_degree=False):\n",
    "         super(DGLDotGatConv, self).__init__(in_feats, out_feats, num_heads, allow_zero_in_degree)\n",
    "         self.graph = graph\n",
    "\n",
    "    def forward(self, feat, get_attention=False):\n",
    "        return super().forward(self.graph, feat, get_attention)\n",
    "\n",
    "class DGLTAGConv(conv.TAGConv):\n",
    "    def __init__(self, graph, in_feats, out_feats, k=2, bias=True, activation=None):\n",
    "         super(DGLTAGConv, self).__init__(in_feats, out_feats, k=k, bias=bias, activation=activation)\n",
    "         self.graph = graph\n",
    "\n",
    "    def forward(self, feat, edge_weight=None):\n",
    "        return super().forward(self.graph, feat, edge_weight)\n",
    "\n",
    "class DGLGATConv(conv.GATConv):\n",
    "    def __init__(\n",
    "            self, graph, in_feats, out_feats, num_heads, \n",
    "            feat_drop=0.0, attn_drop=0.0, negative_slope=0.2, residual=False, \n",
    "            activation=None, allow_zero_in_degree=False, bias=True\n",
    "        ):\n",
    "         super(DGLGATConv, self).__init__(\n",
    "             in_feats, out_feats, num_heads, feat_drop=feat_drop, attn_drop=attn_drop, \n",
    "             negative_slope=negative_slope, residual=residual, activation=activation, \n",
    "             allow_zero_in_degree=allow_zero_in_degree, bias=bias\n",
    "            )\n",
    "         self.graph = graph\n",
    "\n",
    "    def forward(self, feat, get_attention=False):\n",
    "        return super().forward(self.graph, feat, get_attention)\n",
    "\n",
    "class DGLGATv2Conv(conv.GATv2Conv):\n",
    "    def __init__(\n",
    "            self, graph, in_feats, out_feats, num_heads, feat_drop=0.0, attn_drop=0.0, negative_slope=0.2, \n",
    "            residual=False, activation=None, allow_zero_in_degree=False, bias=True, share_weights=False\n",
    "        ):\n",
    "         super(DGLGATv2Conv, self).__init__(\n",
    "             in_feats, out_feats, num_heads, feat_drop=feat_drop, attn_drop=attn_drop, \n",
    "             negative_slope=negative_slope, residual=residual, activation=activation, \n",
    "             allow_zero_in_degree=allow_zero_in_degree, bias=bias, share_weights=share_weights\n",
    "            )\n",
    "         self.graph = graph\n",
    "\n",
    "    def forward(self, feat, get_attention=False):\n",
    "        return super().forward(self.graph, feat, get_attention)\n",
    "\n",
    "class DGLSAGEConv(conv.SAGEConv):\n",
    "    def __init__(self, graph, in_feats, out_feats, aggregator_type, feat_drop=0.0, bias=True, norm=None, activation=None):\n",
    "         super(DGLSAGEConv, self).__init__(\n",
    "             in_feats, out_feats, aggregator_type, feat_drop=feat_drop, \n",
    "             bias=bias, norm=norm, activation=activation\n",
    "            )\n",
    "         self.graph = graph\n",
    "\n",
    "    def forward(self, feat, edge_weight=None):\n",
    "        return super().forward(self.graph, feat, edge_weight)\n",
    "\n",
    "\n",
    "class DGLPNAConv(conv.PNAConv):\n",
    "    def __init__(\n",
    "            self, graph, in_size, out_size, aggregators, scalers, delta, \n",
    "            dropout=0.0, num_towers=1, edge_feat_size=0, residual=True\n",
    "        ):\n",
    "         super(DGLPNAConv, self).__init__(in_size, out_size, aggregators, scalers, delta, dropout=dropout, num_towers=num_towers, edge_feat_size=edge_feat_size, residual=residual)\n",
    "         self.graph = graph\n",
    "\n",
    "    def forward(self, node_feat, edge_feat=None):\n",
    "        return super().forward(self.graph, node_feat, edge_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from gode.utils import is_list_like\n",
    "from dgl.nn.pytorch import GATConv\n",
    "\n",
    "class GODE(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, num_heads, activation=nn.Tanh):\n",
    "        super(GODE, self).__init__()\n",
    "\n",
    "        self.in_feats = in_feats\n",
    "        self.out_feats = out_feats\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        num_heads = num_heads if is_list_like(num_heads) else [num_heads]\n",
    "        \n",
    "        \n",
    "        \n",
    "        for idx, module in enumerate([\n",
    "            GATConv(\n",
    "                in_feats=in_feats, out_feats=out_feats, \n",
    "                num_heads=n_heads, residual=False,\n",
    "                activation=activation if i <= (len(num_heads) - 1) else None, \n",
    "                feat_drop=0.0, \n",
    "                attn_drop=0.0\n",
    "            ) for i, n_heads in enumerate(num_heads)\n",
    "        ]):\n",
    "            self.add_module(str(idx), module)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, graph, feat, get_attention=False):\n",
    "        attns = []\n",
    "        for layer in self:\n",
    "            if get_attention:\n",
    "                feat, attn = layer(graph, feat, get_attention)\n",
    "                attns.append(attn)\n",
    "            else:\n",
    "                feat = layer(graph, feat)\n",
    "        \n",
    "        if get_attention:\n",
    "            return feat, attns\n",
    "        return feat\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
